{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asrulsibaoel/miniconda3/lib/python3.12/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.24). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional, Union, Any\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('aerial_detection.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo11l-seg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AerialModelConfig:\n",
    "    img_size: int = 1024          # Larger size for aerial details\n",
    "    tile_size: int = 1024         # Size for tiling large images\n",
    "    tile_overlap: int = 128       # Overlap between tiles\n",
    "    batch_size: int = 8           # Reduced due to larger images\n",
    "    num_epochs: int = 100\n",
    "    learning_rate: float = 0.01\n",
    "    num_classes: int = 1\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_type: str = \"yolo11l-seg.pt\"  # Larger model for complex features\n",
    "    min_visibility: float = 0.15   # Minimum object visibility threshold\n",
    "    cache_images: bool = True      # Cache images in memory for faster training\n",
    "    save_period: int = 10          # Save checkpoint every N epochs\n",
    "    project_name: str = \"aerial_haul_road_detection\"\n",
    "    experiment_name: str = f\"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProcessor:\n",
    "    \"\"\"Handle preprocessing of aerial/satellite images\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def enhance_contrast(image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply CLAHE contrast enhancement\"\"\"\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "        lab[:, :, 0] = clahe.apply(lab[:, :, 0])\n",
    "        return cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "    @staticmethod\n",
    "    def tile_image(\n",
    "        image: np.ndarray,\n",
    "        tile_size: int,\n",
    "        overlap: int\n",
    "    ) -> List[Tuple[np.ndarray, Tuple[int, int]]]:\n",
    "        \"\"\"Split large images into overlapping tiles\"\"\"\n",
    "        tiles = []\n",
    "        h, w = image.shape[:2]\n",
    "\n",
    "        for y in range(0, h-overlap, tile_size-overlap):\n",
    "            for x in range(0, w-overlap, tile_size-overlap):\n",
    "                end_y = min(y + tile_size, h)\n",
    "                end_x = min(x + tile_size, w)\n",
    "                tile = image[y:end_y, x:end_x]\n",
    "\n",
    "                # Pad if tile is smaller than tile_size\n",
    "                if tile.shape[0] != tile_size or tile.shape[1] != tile_size:\n",
    "                    padded_tile = np.zeros(\n",
    "                        (tile_size, tile_size, 3), dtype=np.uint8)\n",
    "                    padded_tile[:tile.shape[0], :tile.shape[1], :] = tile\n",
    "                    tile = padded_tile\n",
    "\n",
    "                tiles.append((tile, (x, y)))\n",
    "\n",
    "        return tiles\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_predictions(\n",
    "        tiles_predictions: List[Dict[str, Any]],\n",
    "        original_size: Tuple[int, int],\n",
    "        tile_size: int,\n",
    "        overlap: int\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Merge predictions from tiles back to original image size\"\"\"\n",
    "        merged_boxes = []\n",
    "        merged_scores = []\n",
    "        merged_classes = []\n",
    "\n",
    "        for pred, (x_offset, y_offset) in tiles_predictions:\n",
    "            if pred.boxes.xyxy.shape[0] > 0:\n",
    "                # Adjust coordinates based on tile position\n",
    "                boxes = pred.boxes.xyxy.cpu().numpy()\n",
    "                boxes[:, [0, 2]] += x_offset\n",
    "                boxes[:, [1, 3]] += y_offset\n",
    "\n",
    "                # Add predictions\n",
    "                merged_boxes.extend(boxes)\n",
    "                merged_scores.extend(pred.boxes.conf.cpu().numpy())\n",
    "                merged_classes.extend(pred.boxes.cls.cpu().numpy())\n",
    "\n",
    "        # Perform NMS on merged predictions\n",
    "        if merged_boxes:\n",
    "            merged_boxes = np.array(merged_boxes)\n",
    "            merged_scores = np.array(merged_scores)\n",
    "            merged_classes = np.array(merged_classes)\n",
    "\n",
    "            # Convert to YOLO format for NMS\n",
    "            merged_predictions = {\n",
    "                'boxes': torch.from_numpy(merged_boxes),\n",
    "                'scores': torch.from_numpy(merged_scores),\n",
    "                'classes': torch.from_numpy(merged_classes)\n",
    "            }\n",
    "        else:\n",
    "            merged_predictions = {\n",
    "                'boxes': torch.zeros((0, 4)),\n",
    "                'scores': torch.zeros(0),\n",
    "                'classes': torch.zeros(0)\n",
    "            }\n",
    "\n",
    "        return merged_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AerialDataset(Dataset):\n",
    "    \"\"\"Dataset class for aerial/satellite imagery\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_dir: Union[str, Path],\n",
    "        label_dir: Union[str, Path],\n",
    "        config: AerialModelConfig,\n",
    "        transform: Optional[A.Compose] = None,\n",
    "        cache_images: bool = False\n",
    "    ) -> None:\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.label_dir = Path(label_dir)\n",
    "        self.transform = transform\n",
    "        self.config = config\n",
    "        self.cache_images = cache_images\n",
    "        self.image_files = sorted(list(self.image_dir.glob(\"*.jpg\")))\n",
    "        self.cache = {}\n",
    "\n",
    "        if self.cache_images:\n",
    "            self._cache_images()\n",
    "\n",
    "    def _cache_images(self) -> None:\n",
    "        \"\"\"Cache images in memory\"\"\"\n",
    "        logging.info(\"Caching images...\")\n",
    "        for idx in tqdm(range(len(self.image_files))):\n",
    "            img_path = self.image_files[idx]\n",
    "            image = cv2.imread(str(img_path))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            self.cache[idx] = image\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Union[torch.Tensor, str]]:\n",
    "        img_path = self.image_files[idx]\n",
    "        label_path = self.label_dir / f\"{img_path.stem}.txt\"\n",
    "\n",
    "        # Load image\n",
    "        if self.cache_images and idx in self.cache:\n",
    "            image = self.cache[idx].copy()\n",
    "        else:\n",
    "            image = cv2.imread(str(img_path))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Apply contrast enhancement\n",
    "        image = ImageProcessor.enhance_contrast(image)\n",
    "\n",
    "        # Read YOLO format labels\n",
    "        labels = []\n",
    "        if label_path.exists():\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    class_id, x, y, w, h = map(float, line.strip().split())\n",
    "                    labels.append([class_id, x, y, w, h])\n",
    "\n",
    "        labels = np.array(labels) if labels else np.zeros((0, 5))\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(\n",
    "                image=image, bboxes=labels[:, 1:], class_labels=labels[:, 0])\n",
    "            image = transformed['image']\n",
    "            if len(transformed['bboxes']) > 0:\n",
    "                labels = np.column_stack([\n",
    "                    transformed['class_labels'],\n",
    "                    transformed['bboxes']\n",
    "                ])\n",
    "            else:\n",
    "                labels = np.zeros((0, 5))\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'labels': torch.from_numpy(labels).float(),\n",
    "            'image_path': str(img_path)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aerial_transforms(config: AerialModelConfig) -> Tuple[A.Compose, A.Compose]:\n",
    "    \"\"\"Create augmentation pipelines for aerial imagery\"\"\"\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(config.img_size, config.img_size),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=1),\n",
    "            A.GaussianBlur(blur_limit=(3, 7), p=1),\n",
    "            A.MotionBlur(blur_limit=(3, 7), p=1),\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.2,\n",
    "                contrast_limit=0.2,\n",
    "                p=1\n",
    "            ),\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=20,\n",
    "                sat_shift_limit=30,\n",
    "                val_shift_limit=20,\n",
    "                p=1\n",
    "            ),\n",
    "        ], p=0.3),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[255, 255, 255]),\n",
    "        ToTensorV2(),\n",
    "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(config.img_size, config.img_size),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[255, 255, 255]),\n",
    "        ToTensorV2(),\n",
    "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "\n",
    "    return train_transform, val_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AerialTrainer:\n",
    "    \"\"\"Training class for aerial imagery detection\"\"\"\n",
    "\n",
    "    def __init__(self, config: AerialModelConfig) -> None:\n",
    "        self.config = config\n",
    "        self.model = YOLO(config.model_type)\n",
    "        self.save_dir = Path(config.project_name) / config.experiment_name\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def train(self, data_yaml_path: str) -> None:\n",
    "        \"\"\"Train the model with aerial-specific settings\"\"\"\n",
    "        logging.info(f\"Starting training with config: {self.config}\")\n",
    "\n",
    "        results = self.model.train(\n",
    "            data=data_yaml_path,\n",
    "            epochs=self.config.num_epochs,\n",
    "            imgsz=self.config.img_size,\n",
    "            batch=self.config.batch_size,\n",
    "            device=self.config.device,\n",
    "            project=self.config.project_name,\n",
    "            name=self.config.experiment_name,\n",
    "            lr0=self.config.learning_rate,\n",
    "            patience=50,\n",
    "            save_period=self.config.save_period,\n",
    "            # Aerial-specific parameters\n",
    "            mosaic=0.75,\n",
    "            mixup=0.25,\n",
    "            degrees=180,\n",
    "            scale=0.5,\n",
    "            fliplr=0.5,\n",
    "            flipud=0.5,\n",
    "            hsv_h=0.015,\n",
    "            hsv_s=0.7,\n",
    "            hsv_v=0.4,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        # Save training results\n",
    "        self._save_results(results)\n",
    "\n",
    "    def _save_results(self, results: Any) -> None:\n",
    "        \"\"\"Save training results and metrics\"\"\"\n",
    "        results_dict = {\n",
    "            'train/box_loss': results.results_dict['train/box_loss'],\n",
    "            'train/cls_loss': results.results_dict['train/cls_loss'],\n",
    "            'val/box_loss': results.results_dict['val/box_loss'],\n",
    "            'val/cls_loss': results.results_dict['val/cls_loss'],\n",
    "            'metrics/precision': results.results_dict['metrics/precision'],\n",
    "            'metrics/recall': results.results_dict['metrics/recall'],\n",
    "            'metrics/mAP50': results.results_dict['metrics/mAP50'],\n",
    "            'metrics/mAP50-95': results.results_dict['metrics/mAP50-95']\n",
    "        }\n",
    "\n",
    "        # Save metrics\n",
    "        with open(self.save_dir / 'results.json', 'w') as f:\n",
    "            json.dump(results_dict, f, indent=4)\n",
    "\n",
    "        # Plot training results\n",
    "        self._plot_results(results_dict)\n",
    "\n",
    "    def _plot_results(self, results: Dict[str, List[float]]) -> None:\n",
    "        \"\"\"Plot training metrics\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "        # Plot losses\n",
    "        axes[0, 0].plot(results['train/box_loss'], label='Train Box Loss')\n",
    "        axes[0, 0].plot(results['val/box_loss'], label='Val Box Loss')\n",
    "        axes[0, 0].set_title('Box Loss')\n",
    "        axes[0, 0].legend()\n",
    "\n",
    "        axes[0, 1].plot(results['train/cls_loss'], label='Train Cls Loss')\n",
    "        axes[0, 1].plot(results['val/cls_loss'], label='Val Cls Loss')\n",
    "        axes[0, 1].set_title('Classification Loss')\n",
    "        axes[0, 1].legend()\n",
    "\n",
    "        # Plot metrics\n",
    "        axes[1, 0].plot(results['metrics/precision'], label='Precision')\n",
    "        axes[1, 0].plot(results['metrics/recall'], label='Recall')\n",
    "        axes[1, 0].set_title('Precision & Recall')\n",
    "        axes[1, 0].legend()\n",
    "\n",
    "        axes[1, 1].plot(results['metrics/mAP50'], label='mAP50')\n",
    "        axes[1, 1].plot(results['metrics/mAP50-95'], label='mAP50-95')\n",
    "        axes[1, 1].set_title('mAP')\n",
    "        axes[1, 1].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.save_dir / 'training_plots.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_yaml(\n",
    "    train_path: str,\n",
    "    val_path: str,\n",
    "    class_names: List[str],\n",
    "    output_path: str = \"dataset.yaml\"\n",
    ") -> None:\n",
    "    \"\"\"Create YAML configuration file for training\"\"\"\n",
    "    data_yaml = {\n",
    "        'train': train_path,\n",
    "        'val': val_path,\n",
    "        'names': {i: name for i, name in enumerate(class_names)},\n",
    "        'nc': len(class_names)\n",
    "    }\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        yaml.dump(data_yaml, f, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 15:47:12,855 - INFO - Starting aerial haul road detection pipeline with config: AerialModelConfig(img_size=1024, tile_size=1024, tile_overlap=128, batch_size=8, num_epochs=100, learning_rate=0.01, num_classes=1, device='cuda', model_type='yolo11l-seg.pt', min_visibility=0.15, cache_images=True, save_period=10, project_name='aerial_haul_road_detection', experiment_name='exp_20250116_154712')\n",
      "2025-01-16 15:47:12,913 - INFO - Caching images...\n",
      "100%|██████████| 4980/4980 [02:52<00:00, 28.82it/s]\n",
      "2025-01-16 15:50:05,912 - INFO - Caching images...\n",
      "100%|██████████| 1246/1246 [00:47<00:00, 26.36it/s]\n",
      "2025-01-16 15:50:59,871 - INFO - Starting training with config: AerialModelConfig(img_size=1024, tile_size=1024, tile_overlap=128, batch_size=8, num_epochs=100, learning_rate=0.01, num_classes=1, device='cuda', model_type='yolo11l-seg.pt', min_visibility=0.15, cache_images=True, save_period=10, project_name='aerial_haul_road_detection', experiment_name='exp_20250116_154712')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.61 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.55 🚀 Python-3.12.8 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=segment, mode=train, model=yolo11l-seg.pt, data=aerial_dataset.yaml, epochs=100, time=None, patience=50, batch=8, imgsz=1024, save=True, save_period=10, cache=False, device=cuda, workers=8, project=aerial_haul_road_detection, name=exp_20250116_1547122, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=180, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.5, fliplr=0.5, bgr=0.0, mosaic=0.75, mixup=0.25, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=aerial_haul_road_detection/exp_20250116_1547122\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  2    173824  ultralytics.nn.modules.block.C3k2            [128, 256, 2, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  2    691712  ultralytics.nn.modules.block.C3k2            [256, 512, 2, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  2   1455616  ultralytics.nn.modules.block.C2PSA           [512, 512, 2]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  2    756736  ultralytics.nn.modules.block.C3k2            [1024, 256, 2, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  2   2365440  ultralytics.nn.modules.block.C3k2            [768, 512, 2, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 23        [16, 19, 22]  1   3718003  ultralytics.nn.modules.head.Segment          [1, 32, 256, [256, 512, 512]] \n",
      "YOLO11l-seg summary: 667 layers, 27,617,459 parameters, 27,617,443 gradients, 142.7 GFLOPs\n",
      "\n",
      "Transferred 1071/1077 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /mnt/e/pekerjaan/simas/road/data/splitted/datasets/labels/train.cache... 0 images, 4980 backgrounds, 0 corrupt: 100%|██████████| 4980/4980 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ No labels found in /mnt/e/pekerjaan/simas/road/data/splitted/datasets/labels/train.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/e/pekerjaan/simas/road/data/splitted/datasets/labels/test.cache... 0 images, 1246 backgrounds, 0 corrupt: 100%|██████████| 1246/1246 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ No labels found in /mnt/e/pekerjaan/simas/road/data/splitted/datasets/labels/test.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to aerial_haul_road_detection/exp_20250116_1547122/labels.jpg... \n",
      "zero-size array to reduction operation maximum which has no identity\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 176 weight(decay=0.0), 187 weight(decay=0.0005), 186 bias(decay=0.0)\n",
      "Image sizes 1024 train, 1024 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1maerial_haul_road_detection/exp_20250116_1547122\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/100      14.2G          0          0        128          0          0       1024:   0%|          | 2/623 [02:08<9:57:56, 57.77s/it] "
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "config = AerialModelConfig()\n",
    "\n",
    "# Setup logging\n",
    "logging.info(\n",
    "    f\"Starting aerial haul road detection pipeline with config: {config}\")\n",
    "\n",
    "# Create transforms\n",
    "train_transform, val_transform = create_aerial_transforms(config)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AerialDataset(\n",
    "    image_dir=\"data/splitted/datasets/images/train\",\n",
    "    label_dir=\"data/splitted/datasets/labels/train\",\n",
    "    config=config,\n",
    "    transform=train_transform,\n",
    "    cache_images=config.cache_images\n",
    ")\n",
    "\n",
    "val_dataset = AerialDataset(\n",
    "    image_dir=\"data/splitted/datasets/images/test\",\n",
    "    label_dir=\"data/splitted/datasets/labels/test\",\n",
    "    config=config,\n",
    "    transform=val_transform,\n",
    "    cache_images=config.cache_images\n",
    ")\n",
    "\n",
    "# Create YAML config\n",
    "class_names = [\"haul_road\"]\n",
    "create_dataset_yaml(\n",
    "    train_path=str(Path(\"data/splitted/datasets/images/train\").absolute()),\n",
    "    val_path=str(Path(\"data/splitted/datasets/images/test\").absolute()),\n",
    "    class_names=class_names,\n",
    "    output_path=\"aerial_dataset.yaml\"\n",
    ")\n",
    "\n",
    "# Initialize trainer and train\n",
    "trainer = AerialTrainer(config)\n",
    "trainer.train(data_yaml_path=\"aerial_dataset.yaml\")\n",
    "\n",
    "    # # Example of prediction on a large image\n",
    "    # predictor = Predictor(\n",
    "    #     model_path=f\"{\n",
    "    #         config.project_name}/{config.experiment_name}/weights/best.pt\",\n",
    "    #     config=config\n",
    "    # )\n",
    "\n",
    "    # # Predict on a test image\n",
    "    # test_image_path = \"path/to/test/image.jpg\"\n",
    "    # predictions = predictor.predict_large_image(\n",
    "    #     test_image_path,\n",
    "    #     conf_threshold=0.25,\n",
    "    #     iou_threshold=0.45\n",
    "    # )\n",
    "\n",
    "    # # Visualize results\n",
    "    # predictor.visualize_predictions(\n",
    "    #     test_image_path,\n",
    "    #     predictions,\n",
    "    #     output_path=f\"\"\"{\n",
    "    #         config.project_name}/{config.experiment_name}/predictions/test_prediction.jpg\"\"\",\n",
    "    #     class_names=class_names\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
