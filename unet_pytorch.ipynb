{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/splitted/deepglobe-2018-dataset\"\n",
    "TRAIN_PATH = \"train/\"\n",
    "VAL_PATH = \"val/\"\n",
    "\n",
    "ENCODER_NAME = \"resnet34\"\n",
    "ENCODER_WEIGHTS = \"imagenet\"\n",
    "\n",
    "# hyper parameters\n",
    "MAX_EPOCHS = 50\n",
    "GPUS = 1\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "1. Download and Explore the Dataset:  \n",
    "- Download the dataset from the provided Codalab competition link.\n",
    "- Explore the data structure, including image dimensions, file formats, and corresponding labels.\n",
    "\n",
    "2. Data Augmentation:  \n",
    "- Use augmentations like rotations, flips, and brightness adjustments to increase the dataset's variety. Leverage libraries like `torchvision.transforms` or `Albumentations`.\n",
    "\n",
    "3. Dataset Class:  \n",
    "- Create a PyTorch dataset class that loads images and masks, applies augmentations, and prepares data for model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HaulRoadDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_dir (str): Path to the directory containing images.\n",
    "            masks_dir (str): Path to the directory containing masks.\n",
    "            transform (callable, optional): Albumentations transformations.\n",
    "        \"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transform = transform\n",
    "        self.images = sorted(os.listdir(images_dir))\n",
    "        self.masks = sorted(os.listdir(masks_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask\n",
    "        img_path = os.path.join(self.images_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"))\n",
    "        mask = (mask > 127).astype(np.float32)\n",
    "\n",
    "        # Apply Albumentations transformations\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Add a channel dimension to the mask\n",
    "        mask = mask[None, ...]  # Shape: [1, H, W]\n",
    "\n",
    "        return image, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "1. Backbone Model:  \n",
    "Use a segmentation model like U-Net, DeepLabV3, or SegFormer. Libraries like segmentation_models_pytorch can help simplify this step.\n",
    "\n",
    "2. Customize for Road Segmentation:  \n",
    "Choose an appropriate backbone (e.g., ResNet, EfficientNet) and adjust output classes for binary segmentation (road vs. background).\n",
    "\n",
    "3. PyTorch Lightning Module:  \n",
    "Wrap your model into a Lightning module for better structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchmetrics import MeanMetric\n",
    "\n",
    "\n",
    "class HaulRoadSegmentationModel(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.lr = lr\n",
    "        self.train_loss = MeanMetric()\n",
    "        self.val_loss = MeanMetric()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        images = images.float()\n",
    "        masks = masks.float()\n",
    "        outputs = self(images)  # Forward pass\n",
    "        loss = self.loss_fn(outputs, masks)\n",
    "        self.train_loss.update(loss)\n",
    "        self.log(\"train_loss\", self.train_loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        images = images.float()\n",
    "        masks = masks.float()\n",
    "        outputs = self(images)\n",
    "        loss = self.loss_fn(outputs, masks)\n",
    "        self.val_loss.update(loss)\n",
    "        self.log(\"val_loss\", self.val_loss, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import label\n",
    "\n",
    "def calculate_road_size(mask, pixel_to_meter_ratio):\n",
    "    labeled_array, num_features = label(mask)\n",
    "    road_sizes = []\n",
    "    for i in range(1, num_features + 1):\n",
    "        component = (labeled_array == i).astype(np.uint8)\n",
    "        road_size = component.sum() * pixel_to_meter_ratio**2\n",
    "        road_sizes.append(road_size)\n",
    "    return road_sizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "1. Data Loaders:  \n",
    "- Create PyTorch data loaders for train, validation, and test splits.\n",
    "\n",
    "2. Training Loop:  \n",
    "- Use PyTorch Lightningâ€™s Trainer to handle the training loop and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# Define the base segmentation model\n",
    "base_model = smp.Unet(\n",
    "    encoder_name=ENCODER_NAME,  # Choose your backbone\n",
    "    encoder_weights=ENCODER_WEIGHTS,  # Pre-trained on ImageNet\n",
    "    in_channels=3,  # Input channels (e.g., RGB images)\n",
    "    classes=1,  # Output channels (binary segmentation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = A.Compose([\n",
    "    A.Resize(256, 256),  # Resize images and masks\n",
    "    A.HorizontalFlip(p=0.5),  # Random horizontal flip\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize\n",
    "    ToTensorV2()  # Convert to PyTorch tensors\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "class HaulRoadDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_dir, val_dir, batch_size=16, transform=None):\n",
    "        super().__init__()\n",
    "        self.train_dir = train_dir\n",
    "        self.val_dir = val_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = HaulRoadDataset(\n",
    "            images_dir=os.path.join(self.train_dir, \"images\"),\n",
    "            masks_dir=os.path.join(self.train_dir, \"masks\"),\n",
    "            transform=self.transform,\n",
    "        )\n",
    "        self.val_dataset = HaulRoadDataset(\n",
    "            images_dir=os.path.join(self.val_dir, \"images\"),\n",
    "            masks_dir=os.path.join(self.val_dir, \"masks\"),\n",
    "            transform=self.transform,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset instances\n",
    "train_dataset = HaulRoadDataset(\n",
    "    images_dir=os.path.join(DATA_PATH, TRAIN_PATH, \"images/\"),\n",
    "    masks_dir=os.path.join(DATA_PATH, TRAIN_PATH, \"masks/\"),\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset = HaulRoadDataset(\n",
    "    images_dir=os.path.join(DATA_PATH, VAL_PATH, \"images/\"),\n",
    "    masks_dir=os.path.join(DATA_PATH, VAL_PATH, \"masks/\"),\n",
    "    transform=transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type              | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model      | Unet              | 24.4 M | train\n",
      "1 | loss_fn    | BCEWithLogitsLoss | 0      | train\n",
      "2 | train_loss | MeanMetric        | 0      | train\n",
      "3 | val_loss   | MeanMetric        | 0      | train\n",
      "---------------------------------------------------------\n",
      "24.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.4 M    Total params\n",
      "97.745    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5538fa0ff4264617879c856b8bea5d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asrulsibaoel/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "/home/asrulsibaoel/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "/home/asrulsibaoel/miniconda3/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (41) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068a70dce20f4330a88216a079254831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f578991bee814073852feea36c72bb4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f213d0b6444d51ae966fcf48ab775b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbafeb48dbf4944a4696c25d598507e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c897a28259644859c967d615b9ac84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f28a0c25974286b091a6fc52e36775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0875955f006646e2a96004aa14d22d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05ab559666948cbae2ef1b79bca0543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ce2365ad774dc4bd74c5d9c5df9bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Define paths\n",
    "train_dir = os.path.join(DATA_PATH, TRAIN_PATH)\n",
    "val_dir = os.path.join(DATA_PATH, VAL_PATH)\n",
    "\n",
    "# Initialize data module\n",
    "data_module = HaulRoadDataModule(\n",
    "    train_dir=train_dir,\n",
    "    val_dir=val_dir,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# # Define model (replace UNet with your custom architecture if applicable)\n",
    "# from torchvision.models.segmentation import fcn_resnet50\n",
    "# model = fcn_resnet50(pretrained=False, num_classes=1)\n",
    "\n",
    "# Lightning model\n",
    "lit_model = HaulRoadSegmentationModel(model=base_model)\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\")\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    accelerator=\"auto\",\n",
    "    devices=GPUS\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.fit(lit_model, datamodule=data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pytorch_lightning.trainer.trainer.Trainer at 0x7fb454612450>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
